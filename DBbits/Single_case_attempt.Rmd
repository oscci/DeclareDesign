---
title: "single_case_eg"
author: "Dorothy Bishop"
date: "started 13/09/2021, today 2/11/2021"
output: html_document
---

<!--- incorporating changes from Macartan Humphreys-->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
library(knitr)
library(kableExtra)
library(gridExtra)
library(DeclareDesign)
library(tidyverse)
library(fabricatr)
library(randomizr)
library(estimatr)
library(psych)
```

## Single case experimental designs (SCEDs)

In some disciplines, notably those where intervention is used with populations that are highly heterogeneous, single case experimental designs (SCEDs) are a set of procedures that are widely used in preference to randomised controlled trials (RCTs). At the extremes, these are radically different approaches to evaluating an intervention, using within-subjects versus between-subjects analyses respectively. The term 'single case' can be misleading; SCEDs are not case reports; they use experimental control and often involve more than one participant, creating a case series. The emphasis, however, is on individual outcomes rather than the group average. 

@best2019 compared the advantages and disadvantages of the two approaches, noting differences in cost and scale (substantially larger for RCTs), as well as different approaches to experimental controls - via use of a control group in RCTs, as opposed to contrasts between time periods or outcomes in SCEDs. Best noted the differing conclusions that can be drawn from RCTs, where "If a difference between groups is found, this is in the group average; the intervention is not necessarily beneficial for all those that are treated", as opposed to SCEDs, where "The findings are applicable on a case by case basis and are analysed for each participant separately."  

This kind of weighing up of pros and cons of different experimental designs is precisely what DeclareDesign aims to do, within a formal framework that requires specification of the underlying assumptions of different approaches, and uses simulated data to compare them on a set of criteria specified by the researcher. The goal is to evaluate designs so that they can be optimised for a given research question. This becomes particularly interesting when one considers that the polarised distinction between RCTs and SCEDs is not absolute, and there is growing interest in combining the two approaches in hybrid designs.  

Here I aim to apply the MIDA approach of DeclareDesign to SCEDs. MIDA stands for Model, Inquiry, Data Strategy and Answer Strategy, with the first two elements being theoretical and the second two empirical. This way of conceptualising intervention studies is quite foreign to most researchers, who tend to focus simply on selecting a research design and analytic method. Its value, however, becomes apparent as soon as we look in detail at the comparisons of methods laid out by @best2019, because it allows us to categorise the different aspects of the two study types as belonging to specific parts of the overall research design.  

I will take as my starting point a study by @calder2021.  This is described as a RCT, but is actually a hybrid design that grew out of previous SCED studies by the authors on the same topic, which is evaluation of an intervention to help language-disordered children use grammatical endings correctly. The design of the study can be grasped by showing the plot of results. 

```{r calderfig,echo=F,eval=F,warning=F,message=F,fig.leg = "Mean % correct for two groups in Calder et al, 2021. Data plotted from Table 2."}
#Original plot created for intervention book
knitr::include_graphics("calderfig.png")
```

The study adopts a waitlist control design and contains two groups studied across 7 phases.  Both groups are initially assessed during a baseline period (Phases 1-2). Group 1 receives intervention during phase 3 and is then assessed again in phases 4 and 5. Group 2 continues untreated during phases 3-4, which count as an extended baseline, but is treated during phase 5, with post-intervention assessment during phases 6 and 7.  Assessment is made using three type of probe item. Past tense -ed probes assess the grammatical morpheme that was trained in the intervention, whereas 3rd person singular and possessive 's probes assess untrained morphemes. On each occasion, 30 probes of each kind were administered - these were subdivided according to the phonological form of the inflection, but we will not consider that aspect here.  

The design resembles a cross-over RCT, insofar as both groups receive intervention at different periods, first Group 1 and then Group 2, and Group 1 ceases intervention at the point when Group 2 starts it. However, unlike in a standard cross-over design, the expectation is that the effects of intervention will persist after the intervention ceases, so Group 1 is not anticipated to return to baseline levels.

The design also has elements of a SCED. There are two ways in which intervention effects can be estimated within subjects: first, by comparing the baseline and post-intervention periods; second, by comparing percentages correct on probes for trained items (past tense -ed) versus probes for untrained items. If intervention effects were simply placebo effects, we would expect to see gains on all item types.  

I will break this design down into components that can be independently evaluated.  We can then see how including additional components of the design affects its efficiency. Let us start with the within-subjects SCED comparison of performance pre- and post-intervention. We can collapse across the two groups and focus on the comparison of scores for the 2 baseline measures preceding intervention and the 2 post-intervention measures. We will focus solely on the treated past tense -ed outcome. Note that this design on its own is unsatisfactory for guarding against systematic bias from practice or developmental effects, and cannot distinguish a specific intervention effect from a placebo effect. Nevertheless, we will start simple, and then build up from there.  Let us see how we could specify this design in the MIDA framework.

## Using the MIDA framework with a pre-post SCED  

### Model  

@humphreys2021 note that the Model is concerned with "what causes what and how" and that "it includes guesses about how important variables are generated, how things are correlated, and the sequences of events." The Model has much in common with the notion of a hypothesis, but it is much more detailed than a typical hypothesis, because it requires detailed specification of details of the population and outcomes. Furthermore, the Model is not a single hypothesis; rather it includes a range of hypotheses. One would be the null hypothesis, that intervention is ineffective, another that the intervention leads to improvement on trained constructions. Yet another might be that there are nonspecific treatment effects (placebo effects) which lead to improvement on all constructions, whether trained or not.  In addition, we need to specify the characteristics of the population the model applies to: in this case children with Developmental Language Disorder, of a given age range. 

An interesting point here, though, is that models for RCTs and SCEDs diverge in terms of how improvement is conceptualised. For a RCT, the focus is on the average improvement; a Model might state that we expect average improvement of 20% on the score for trained constructions. For SCED the assumption is that there will be heterogeneity in responses that we wish to incorporate in the model. Thus we would postulate that a proportion, p, of individuals are responsive to intervention, whereas the remainder do not respond - so we'd have a mixture of null and non-null results. If we simulate results from these two models, we might find they are hard to tell apart - but DeclareDesign should help us diagnose which methods are best at making such a distinction.

We can also include in the SCED model information about the characteristics of responders. For instance prior research showed that children who have difficulty producing certain speech sounds failed to progress on the intervention used by @calder2021. For now, however, we will keep things simple and just assume the model specifies the population of interest, and the sample size. Initial baseline scores are also included in the model. These vary from person to person. Since 10 items are used for each probe, we can simulate these by selecting a number at random from 0 to 10. Note that @calder2021 excluded two cases who performed at ceiling at baseline; we can use baseline scores to exclude those who score 10. 

A key component of the model is the potential outcomes, which depend on the intervention group. In this simulation everyone receives the intervention, and so all responders should show improvement. We simulate this by adding a treatment effect, tau, to the post-intervention phase for those who are identified as treatment responders.  

Because we have a series of 4 timepoints, two before and two after intervention, we need to consider how these are related. It is realistic to assume that scores will be auto-correlated, i.e. someone who has a high score at time 1 will tend to have a high score at times 2-4, regardless of intervention. We can simulate this situation by specifying a degree of correlation between successive time points.  

For this analysis, we can base these parameters on those observed in real data (raw data kindly provided by Samuel Calder).  

```{r readrawdata,echo=F}
#Calder's data was reformatted using the readjslhr script, to create a .csv file with inflection type and group in long form.  NB One participant with no data at followup is excluded; also one measure of .111 was modified to .1, as .111 not compatible with 30 items.
calderdat <- read.csv('CalderRaw.csv')

#Note also that group 2 had 3 assessments pre intervention and 2 after, whereas group 1 had 2 before and 2 after. No indication of any improvement between sessions 1-2 for group 2, so comparison could be between their 1st 2 and last 2. 

calderdat$pre1 <- calderdat$T1
calderdat$pre2 <- calderdat$T2
calderdat$pre1[calderdat$group==2]<-calderdat$T3[calderdat$group==2]
calderdat$pre2[calderdat$group==2]<-calderdat$T4[calderdat$group==2]
calderdat$post1 <- calderdat$T4
calderdat$post2 <- calderdat$T5
calderdat$post1[calderdat$group==2]<-calderdat$T6[calderdat$group==2]
calderdat$post2[calderdat$group==2]<-calderdat$T7[calderdat$group==2]

w<-which(colnames(calderdat)=='pre1')
cormat <- cor(calderdat[calderdat$morph=='ed',w:(w+3)])
myr <- cormat[2,1] #correlation for -ed items is around .7
calderdat$premean<-(calderdat$pre1+calderdat$pre2)/2
calderdat$postmean<-(calderdat$post1+calderdat$post2)/2
calderdat$change <- calderdat$postmean-calderdat$premean



caldersummary <- describeBy(calderdat[,15:17],group=calderdat$morph,mat=TRUE)
caldersummary

calderdat$groupmorph<-paste0('g',calderdat$group,calderdat$morph)
caldersummary2 <- describeBy(calderdat[,2:8],group=calderdat$groupmorph,mat=TRUE)
caldersummary2

#It turns out that on the -ed structure, everyone improves, so it's obvious even on a simple sign test. 
#On 3s 16/20 improve - so this is a more interesting one to consider in terms of whether one can identify the subset who improve (and distinguish from changes just due to random error)
#On poss 15/20 improve - but some get markedly worse. 

#Just checking if improvement seen within pre or post
calderdat$pre2.1 <-calderdat$pre2-calderdat$pre1
calderdat$post2.1 <-calderdat$post2-calderdat$post1

caldersummary3 <- describeBy(calderdat[,19:20],group=calderdat$morph,mat=TRUE)
caldersummary3

#No evidence of any gains between the two pre-test or two post-test sessions, except for pre on possessives

```


```{r specifygeneralparameters,echo=F}
  p = .5 #proportion who respond to treatment. Can vary this.
  mineffect = 2 #used in definition of treatment effect; n items increase in score
  maxeffect = 15 #used in definition of treatment effect
  startprobvals =c(.2,.7) #initial probability correct will vary from person to person - actual value will be assigned at random by taking value between this range and squaring it - this will give a skewed distribution
  buskcut = 1 #cutoff for categorising using Busk d [?arbitrary value]
  Nmeasures = 3 #N measures pre(baseline) and post-intervention; so far set so can be 2 or 3
  nitem = 30 #N items per inflection
  r = myr #estimated correlation between successive measures without any intervention
  npts <- 2 # N observations pre and posttest
```

```{r model,echo=T}
#Original model with 2 time points for pre and 2 for post
#First decide on the size of population of interest.
# For Calder et al, this might be all the children with DLD in the language units they sampled from, e.g. 150
if (npts==2){
M <- declare_model(

  N = 150, #population size - this is just a guestimate of all the children who *could* have been included
  Z = 1, #in this design, all cases receive intervention 
   tau = round(runif(N, min = mineffect, max = maxeffect),0), # treatment effect - here specified to vary at random for each person from min to max. These values selected to give data that resemble Calder et al (though does depend on proportion of responders as well as max and min)
  #tau = 0,  #set tau to zero to see what results look like if treatment is ineffective for everyone.
  responder = rbinom(N, 1, p), #specifies which cases show treatment effect; this will be a proportion of people based on p.  NB this is a latent variable that we cannot observe, but we try to infer it from the data.
  startprob = runif(N,min=startprobvals[1],max=startprobvals[2])^2, #values are squared to give a skewed distribution
  U = 2*rnorm(N), #random error (U for unknown) #multiplied by 2 just to give realistic amount of variation across people. 

  Y1_1 = rbinom(N,nitems,startprob), #baseline % correct as binomial where startprob is prob of item correct
  Y2_1 = r*Y1_1+sqrt(1-r^2)*U, #Y1_1 is baseline measure, Y2_1 is 2nd baseline, correlates with Y1_1 with correlation r
  Y1_2 = r*Y2_1+sqrt(1-r^2)*U, #Y1_2 is first post-intervention measure, correlates with Y2_1
  Y2_2 = r*Y1_2+sqrt(1-r^2)*U, #Y2_2 is 2nd post-intervention measure, correlates with Y1_2

  #Now do potential outcomes for post-intervention measures - everyone treated, so Z is 1. 
  Y1_2_Z = Y1_2+Z*tau*responder, #treatment effect added to Y1_2
  Y2_2_Z = Y2_2+Z*tau*responder, #treatment effect added to Y2_2
  # ensure all values bounded between 0 and 10 and are whole numbers

  Y2_1 = if_else(Y2_1 > nitem, nitem, round(Y2_1,0)),
  Y1_2_Z = if_else(Y1_2_Z>nitem, nitem, round(Y1_2_Z,0)),
  Y2_2_Z = if_else(Y2_2_Z>nitem, nitem, round(Y2_2_Z,0)),
  Y2_1 = if_else(Y2_1 < 0, 0, Y2_1),
  Y1_2_Z = if_else(Y1_2_Z<0, 0, Y1_2_Z),
  Y2_2_Z = if_else(Y2_2_Z<0, 0, Y2_2_Z),

  
  #Get a measure of how much overlap between scores in baseline and post intervention
   no_overlap = pmax(Y1_1, Y2_1) < pmin( Y1_2_Z, Y2_2_Z), #1 if values from pretest don't overlap with posttest.
   correctcat = ifelse(as.numeric(no_overlap)==responder,1,0), #1 if no_overlap measure correctly categorises as responder or nonresponder. The proportion of correctcat cases indicates how accurate the no_overlap criterion is at categorising people
  # 
  # #We now compare with Busk and Serlin's measure, d2
#    varpooled <-(apply(cbind(Y1_1,Y2_1),1,var)+apply(cbind(Y1_2_Z, Y2_2_Z),1,var))/2,
# NB I originally computed variances in separate steps but fabricatr doesn't like this, 
# hence v complicated formula here.
    busk_d2 = ((Y1_2_Z+Y2_2_Z)-(Y1_1+Y2_1))/2/sqrt((apply(cbind(Y1_1,Y2_1),1,var)+apply(cbind(Y1_2_Z, Y2_2_Z),1,var))/2),
    buskcat = ifelse(busk_d2>buskcut,1,0),  
    correctbusk = ifelse(buskcat==responder,1,0) #1 if busk category and latent responder measure agree 
)
  
#If I want to see what is generated by M, I just type:
m = M() #to get dataframe m.
table(m$responder,m$buskcat)
allcategorised <- sum(table(m$responder,m$buskcat)) #some are NA
print(paste0("Model with 2 periods pre and post, buskcut = ",buskcut,": pcorrect: ",sum(m$correctbusk,na.rm=T)/allcategorised))
}
```



```{r model3pts,echo=T}
#Same as model2pts but with 3 time points for pre and post
#Use same parameters as for model2pts
if (npts==3){
M3 <- declare_model(

  N = 150, #population size - this is just a guestimate of all the children who could have been included
  Z = 1, #in this design, all cases receive intervention 
  W = rbinom(N, 1, .8), # score on speech production test; pass/fail - potential covariate for later on. Ignore here.
  tau = round(runif(N, min = mineffect, max = maxeffect),0), # treatment effect - here specified to vary at random for each person from min to max. These values selected to give data that resemble Calder et al (though does depend on proportion of responders as well as max and min)
  #tau = 0,  #set tau to zero to see what results look like if treatment is ineffective for everyone.
  responder = rbinom(N, 1, p), #specifies which cases show treatment effect; this will be a proportion of people based on p.  NB this is a latent variable that we cannot observe, but we try to infer it from the data.
  U = 2*rnorm(N), #random error (U for unknown) #multiplied by 2 just to give realistic amount of variation across people. 

 
  Y1_1 = rbinom(N,10,startprob), #baseline % correct as binomial where startprob is prob of item correct
  Y2_1 = r*Y1_1+sqrt(1-r^2)*U, #Y1_1 is baseline measure, Y2_1 is 2nd baseline, correlates with Y1_1 with correlation r
  Y3_1 = r*Y2_1+sqrt(1-r^2)*U, #3rd baseline
  Y1_2 = r*Y3_1+sqrt(1-r^2)*U, #Y1_2 is first post-intervention measure, correlates with Y3_1
  Y2_2 = r*Y1_2+sqrt(1-r^2)*U, #Y2_2 is 2nd post-intervention measure, correlates with Y1_2
  Y3_2 = r*Y2_2+sqrt(1-r^2)*U, #Y3_2 is 3rd post-intervention measure, correlates with Y2_2
  #Now do potential outcomes for post-intervention measures - everyone treated, so Z is 1. 
  Y1_2_Z = Y1_2+Z*tau*responder, #treatment effect added to Y1_2
  Y2_2_Z = Y2_2+Z*tau*responder, #treatment effect added to Y2_2
  Y3_2_Z = Y3_2+Z*tau*responder, #treatment effect added to Y3_2
  # ensure all values bounded between 0 and 10 and are whole numbers

  Y2_1 = if_else(Y2_1 > nitem, nitem, round(Y2_1,0)),
  Y3_1 = if_else(Y3_1 > nitem, nitem, round(Y3_1,0)),
  Y1_2_Z = if_else(Y1_2_Z>nitem, nitem, round(Y1_2_Z,0)),
  Y2_2_Z = if_else(Y2_2_Z>nitem, nitem, round(Y2_2_Z,0)),
  Y3_2_Z = if_else(Y3_2_Z>nitem, nitem, round(Y3_2_Z,0)),
  Y2_1 = if_else(Y2_1 < 0, 0, Y2_1),
  Y3_1 = if_else(Y3_1 < 0, 0, Y3_1),
  Y1_2_Z = if_else(Y1_2_Z<0, 0, Y1_2_Z),
  Y2_2_Z = if_else(Y2_2_Z<0, 0, Y2_2_Z),
  Y3_2_Z = if_else(Y3_2_Z<0, 0, Y3_2_Z),
  
  #Get a measure of how much overlap between scores in baseline and post intervention
  no_overlap = pmax(Y1_1, Y2_1,Y3_1) < pmin( Y1_2_Z, Y2_2_Z,Y3_2_Z), #1 if values from pretest don't overlap with posttest.
  correctcat = ifelse(as.numeric(no_overlap)==responder,1,0), #1 if no_overlap measure correctly categorises as responder or nonresponder. The proportion of correctcat cases indicates how accurate the no_overlap criterion is at categorising people
  
#We now compare with Busk and Serlin's measure, d2
#Numerator is mean of post minus mean of pre (baseline), 
#Denominator is pooled sd, i.e. is square root of the weighted average
#of the variances for pre and post.
#(alternative is d1, which has sd of baseline only as denominator. This is reckoned to be better by Beeson et al (2006) but  is problematic if no variation in baseline)
#  varpre <-apply(cbind(Y1_1, Y2_1,Y3_1),1,var), #variance by rows for baseline
#  varpost <- apply(cbind(Y1_2_Z, Y2_2_Z,Y3_2_Z),1,var), #variance by rows for post
 # varpooled <-(apply(cbind(Y1_1,Y2_1,Y3_1),1,var)+apply(cbind(Y1_2_Z, Y2_2_Z,Y3_2_Z),1,var))/2,
# NB if lines above for explicit computation of pooled variance are included, script doesn't run.
   busk_d2 = ((Y1_2_Z+Y2_2_Z+Y3_2_Z)-(Y1_1+Y2_1+Y3_1))/3/sqrt((apply(cbind(Y1_1,Y2_1,Y3_1),1,var)+apply(cbind(Y1_2_Z, Y2_2_Z,Y3_2_Z),1,var))/2),
   buskcat = ifelse(busk_d2>buskcut,1,0),  
   correctbusk = ifelse(buskcat==responder,1,0) #1 if busk category and latent responder measure agree 
)
  
#If I want to check what is generated by M, I just type:
m = M3() #to get dataframe m.
table(m$responder,m$buskcat)
allcategorised <- sum(table(m$responder,m$buskcat)) #some are NA
print(paste0("Model with 3 periods pre and post, buskcut = ",buskcut,": pcorrect: ",sum(m$correctbusk,na.rm=T)/allcategorised))
}
```

### Inquiry  
The second theoretical component of the design is the Inquiry, i.e. the question that we ask of the Model. In an RCT, it is usually "what is the average effect of intervention?". In a SCED, however, we ask which individuals benefited from the intervention.  We have to decide on a criterion to use to judge who improved. This has been a topic of considerable debate in the literature on SCEDs. If we have only two baseline measures and two post-intervention measures, as in this example, we could use a very simple index, i.e. whether there is any overlap between the two sets of scores. An alternative option would be to take a numerical cutoff, e.g. whether the mean post-intervention cutoff is at least 25% higher than the pre-intervention cutoff.  We show here the Inquiry for the first of these options.

```{r inquiryA, echo=T}

#Our interest is in 
# a) what proportion of cases are identified as responders by a given categorisation rule
# b) what proportion of cases are *corrrectly* categorised by our categorisation rule.
# Catgorisation rule 1 is whether there is any overlap between the pre-intervention scores and the postintervention scores.
# Catgorisation rule 2 is Busk & Serlin d2
#We used the rule to infer whether the person is a responder or non-responder : the responder variable is a latent variable that determines outcomes but is not directly observable. 
#Only a proportion of people are responders. 
I <- declare_inquiry(
  mean_no_ol = mean(no_overlap),
  meancorrcat = mean(correctcat),
  meanbusk = mean(buskcat),
  meancorrbusk= mean(correctbusk)
  )


M() %>% I() #this displays estimand from M() - will differ on each run

#Or can set i <- I(M()) and then look at i
```

Both of these criteria for improvement might seem like rather inadequate and unsophisticated methods. DeclareDesign allows us to test whether such misgivings are justified, because we can see exactly how our Inquiry performs under various scenarios specified in the model. In particular, we can check:  
- How often does the method lead us to conclude that a person has improved with intervention when the null hypothesis is true, i.e. there is no benefit from intervention. This is the False Positive rate.  
- How often does the method correctly indicate which persons have improved with intervention when tau has been set to a nonzero value. This is analogous to the power of the approach.  

We can also check how far the performance of the method improves if we modify the Model to include more datapoints, e.g. if there are 3 or 4 measures at baseline and/or post-intervention.  

### Data Strategy  

```{r dataA,echo=T}
# Sample from the population defined in M; Calder et al had 23 participants
Sall <- 23

sampling_handler = function(data)
  data %>%
  dplyr::filter(Y1_1 < 10) %>%
  dplyr::filter(complete_rs(n(), Sall)==1)
  

D <- declare_sampling(handler = sampling_handler)

#to see a sample of data type
# D(m)
```

### Answer strategy  
The answer strategy is applied to the empirical data to answer the inquiry. 
There are numerous ways in which an answer can be determined, most of which involve applying a standard statistical test. In our simple example, however, we use a very crude index and so do not need to do more than count.  

```{r answerA,echo=T}
A1 <- 
   declare_estimator( no_overlap ~ 1, model = lm_robust, inquiry = "mean_no_ol",label="mean_no_ol")  #This is responder/nonresponder categorisation correct by overlap criterion

A2<-  declare_estimator(correctcat ~ 1, model = lm_robust, inquiry = "meancorrcat",label="meancorrcat") 
 #This is responder/nonresponder categorisation correct by overlap criterion
A3 <- declare_estimator( buskcat ~ 1, model = lm_robust, inquiry = "meanbusk",label="meanbusk")  #This is responder/nonresponder categorisation by busk criterion
A4 <-  declare_estimator(
  correctbusk ~ 1, model = lm_robust, inquiry = "meancorrbusk",label="meancorrbusk") #This is correct categorisation by busk


#to see results of this step:
#A(D(m))



```


```{r design,echo=T}
design <-
  M +
  I +
  D +
  A1+A2+A3+A4 

mydraw <- draw_data(design) #single simulated dataset

#Plot results of single draw to check whether they look like real data
plotcols <- c("Y1_1","Y2_1","Y1_2_Z","Y2_2_Z")
w<-which(colnames(mydraw)%in% plotcols)

#when plotting we divide Y values by nitem to give proportion, so easier to compare with original data
yvals <- c(mean(mydraw$Y1_1),mean(mydraw$Y2_1),mean(mydraw$Y1_2_Z),mean(mydraw$Y2_2_Z))/nitem
plot(1:4,yvals,type='b',xaxt = "n",ylab="Mean score",xlab="",ylim=c(0,.6),main='Sample plot of results',lwd=3)
for (i in 1:nrow(mydraw)){
  lines(1:4,mydraw[i,w]/nitem,type='l',lty=2,col=1+(2*mydraw$correctbusk[i]+mydraw$responder[i]) )

  }
legend("topleft", legend=c("Misclassified nonresp", "Misclassified resp","Correct nonresp","Correct resp"),
       col=1:4, lty=1, cex=0.8)
axis(1, at = 1:4)
text(1.5,0,'Pre-intervention',cex=.8)
text(3.5,0,'Post-intervention',cex=.8)

#The plot colour codes each person, according to whether they really are a responder or nonresponder, and how they are classified by the busk criterion. So Misclassified resp is a responder who was misclassified as non-responder

## How are overlap and responder status related?
## Of course overall depends on both treatment effects and responder status

#This table based on most recent draw: indicates how many improved, as well as how good correspondence is between the true (latent) status of individuals as responders, and their identification as an improved case (with no overlap). 
myt <- table(mydraw$responder, mydraw$no_overlap)
row.names(myt)<-c('Non-responder','Responder')
colnames(myt)<-c('Overlap','No overlap')
myt

myt <- table(mydraw$responder, mydraw$buskcat)
row.names(myt)<-c('Non-responder','Responder')
colnames(myt)<-c('Busk0','Busk1')
myt

draw_estimates(design) %>% kable()



## Below is the code that makes the default diagnosands.
# You can use these as a model when writing your own diagnosands.

# default_diagnosands <- declare_diagnosands(
# bias = mean(estimate - estimand),
# rmse = sqrt(mean((estimate - estimand) ^ 2)),
# power = mean(p.value < alpha),
# coverage = mean(estimand <= conf.high & estimand >= conf.low),
# mean_estimate = mean(estimate),
# sd_estimate = sd(estimate),
# mean_se = mean(std.error),
# type_s_rate = mean((sign(estimate) != sign(estimand))[p.value < alpha]),
# mean_estimand = mean(estimand)
# )

declare_diagnosands(
bias = mean(estimate - estimand),
rmse = sqrt(mean((estimate - estimand) ^ 2)),
power = mean(p.value < alpha),
mean_estimate = mean(estimate),
sd_estimate = sd(estimate),
mean_estimand = mean(estimand))




#To see repeated estimates from design:
#simulations <- simulate_design(design, sims = 100)


diagnose_design(design) %>% reshape_diagnosis(select=c('Inquiry','N Sims','Bias','Mean Estimate','SD Estimate','Mean Estimand')) %>% kable()
```

Now we can tweak aspects of model to see the effect

```{r tweakmodel}
dotweak <-0 #make this optional as it is slow
if(dotweak==1){
designs <- redesign(design,
                    p=seq(.3,.9,.2),
                    startprob=seq(.1,.5,.1))
diagnosis <- diagnose_designs(designs)
}

```


