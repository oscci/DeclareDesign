---
title: "single_case_eg"
author: "Dorothy Bishop"
date: "13/09/2021"
output: html_document
---

<!--- incorporating changes from Macartan Humphreys-->
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
options(digits=2)
library(knitr)
library(kableExtra)
library(gridExtra)
library(DeclareDesign)
library(tidyverse)
library(fabricatr)
library(randomizr)
library(estimatr)
```

## Single case experimental designs (SCEDs)

In some disciplines, notably those where intervention is used with populations that are highly heterogeneous, single case experimental designs (SCEDs) are a set of procedures that are widely used in preference to randomised controlled trials (RCTs). At the extremes, these are radically different approaches to evaluating an intervention, using within-subjects versus between-subjects approaches to analysis respectively. The term 'single case' can be misleading; SCEDs are not case reports; they use experimental control and often involve more than one participant, creating a case series. The emphasis, however, is on individual outcomes rather than the group average. 

@best2019 compared the advantages and disadvantages of the two approaches, noting differences in cost and scale (substantially larger for RCTs), as well as different approaches to experimental controls - via use of a control group in RCTs, as opposed to contrasts between time periods or outcomes in SCEDs. Of particular interest here are the differing conclusions that can be drawn from RCTs, where "If a difference between groups is found, this is in the group average; the intervention is not necessarily beneficial for all those that are treated", as opposed to SCEDs, where "The findings are applicable on a case by case basis and are analysed for each participant separately."  

This kind of weighing up of pros and cons of different experimental designs is precisely what DeclareDesign aims to do, within a formal framework that requires specification of the underlying assumptions of different approaches, and uses simulated data to compare them on a set of criteria specified by the researcher. The goal is to evaluate designs so that they can be optimised for a given research question. This becomes particularly interesting when one considers that the polarised distinction between RCTs and SCEDs is not absolute, and there is growing interest in combining the two approaches in hybrid designs.  

Here I aim to apply the MIDA approach of DeclareDesign to SCEDs. MIDA stands for Model, Inquiry, Data Strategy and Answer Strategy, with the first two elements being theoretical and the second two empirical. This way of conceptualising intervention studies is quite foreign to most researchers, who tend to focus simply on selecting a research design and analytic method. Its value, however, becomes apparent as soon as we look in detail at the comparisons of methods laid out by @best2019, because it allows us to categorise the different aspects of the two study types as belonging to specific parts of the overall research design.  

I will take as my starting point a study by @calder2021.  This is described as a RCT, but is actually a hybrid design that grew out of previous SCED studies by the authors on the same topic, which is evaluation of an intervention to help language-disordered children use grammatical endings correctly. The design of the study can be grasped by showing the plot of results. 

```{r calderfig,echo=F,eval=F,warning=F,message=F,fig.leg = "Mean % correct for two groups in Calder et al, 2021. Data plotted from Table 2."}
#Original plot created for intervention book
knitr::include_graphics("calderfig.png")
```

The study adopts a waitlist control design and contains two groups studied across 7 phases.  Both groups are initially assessed during a baseline period (Phases 1-2). Group 1 receives intervention during phase 3 and is then assessed again in phases 4 and 5. Group 2 continues untreated during phases 3-4, which count as an extended baseline, but is treated during phase 5, with post-intervention assessment during phases 6 and 7.  Assessment is made using three type of probe item. Past tense -ed probes assess the grammatical morpheme that was trained in the intervention, whereas 3rd person singular and possessive 's probes assess untrained morphemes. On each occasion, 10 probes of each kind were administered.  

The design resembles a cross-over RCT, insofar as both groups receive intervention at different periods, first Group 1 and then Group 2, and Group 1 ceases intervention at the point when Group 2 starts it. However, unlike in a standard cross-over design, the expectation is that the effects of intervention will persist after the intervention ceases, so Group 1 is not anticipated to return to baseline levels. This complicates the analysis.  

The design also has elements of a SCED. There are two ways in which intervention effects can be estimated within subjects: first, by comparing the baseline and post-intervention periods; second, by comparing percentages correct on probes for trained items (past tense -ed) versus probes for untrained items. If intervention effects were simply placebo effects, we would expect to see gains on all item types.  

To start simple, I will break this design down into components that can be independently evaluated.  We can then see how including additional components of the design affects its efficiency. Let us start with the within-subjects SCED comparison of performance pre- and post-intervention. We can collapse across the two groups and focus on the comparison of scores for the 2 baseline measures preceding intervention and the 2 post-intervention measures. We will focus solely on the treated past tense -ed outcome. Note that this design on its own is unsatisfactory for guarding against systematic bias from practice or developmental effects, and cannot distinguish a specific intervention effect from a placebo effect. Nevertheless, we will start simple, and then build up from there.  Let us see how we could specify this design in the MIDA framework.

## Using the MIDA framework with a pre-post SCED  

### Model  

@humphreys2021 note that the Model is concerned with "what causes what and how" and that "it includes guesses about how important variables are generated, how things are correlated, and the sequences of events." The Model has much in common with the notion of a hypothesis, but it is much more detailed than a typical hypothesis, because it requires detailed specification of details of the population and outcomes. Furthermore, the Model is not a single hypothesis; rather it includes a range of hypotheses. One would be the null hypothesis, that intervention is ineffective, another that the intervention leads to improvement on trained constructions. Yet another might be that there are nonspecific treatment effects (placebo effects) which lead to improvement on all constructions, whether trained or not.  In addition, we need to specify the characteristics of the population the model applies to: in this case children with Developmental Language Disorder, of a given age range. 

An interesting point here, though, is that models for RCTs and SCEDs diverge in terms of how improvement is conceptualised. For a RCT, the focus is on the average improvement; a Model might state that we expect average improvement of 20% on the score for trained constructions. For SCED the assumption is that there will be heterogeneity in responses that we wish to incorporate in the model. Thus we would postulate that a proportion, p, of individuals are responsive to intervention, whereas the remainder do not respond - so we'd have a mixture of null and non-null results. If we simulate results from these two models, we might find they are hard to tell apart - but DeclareDesign should help us diagnose which methods are best at making such a distinction.

We can also include in the SCED model information about the characteristics of responders. For instance prior research showed that children who have difficulty producing certain speech sounds failed to progress on the intervention used by @calder2021. For now, however, we will keep things simple and just assume the model specifies the population of interest, and the sample size. Initial baseline scores are also included in the model. These vary from person to person. Since 10 items are used for each probe, we can simulate these by selecting a number at random from 0 to 10. Note that @calder2021 excluded two cases who performed at ceiling at baseline; we can use baseline scores to exclude those who score 10. 

A key component of the model is the potential outcomes, which depend on the intervention group. In this simulation everyone receives the intervention, and so all responders should show improvement. We simulate this by adding a treatment effect, tau, to the post-intervention phase for those who are identified as treatment responders.  

Because we have a series of 4 timepoints, two before and two after intervention, we need to consider how these are related. It is realistic to assume that scores will be auto-correlated, i.e. someone who has a high score at time 1 will tend to have a high score at times 2-4, regardless of intervention. We can simulate this situation by specifying a degree of correlation between successive time points. 


```{r modelA,echo=T}
#First decide on the size of population of interest.
# For Calder et al, this might be all the children with DLD in the language units they sampled from, e.g. 150
  p = .5 #proportion who respond to treatment. Can vary this.
  mineffect = 3 #used in definition of treatment effect
  maxeffect = 5 #used in definition of treatment effect
  startprob =.3

M <- declare_model(

  N = 150, #population size - this is just a guestimate of all the children who could have been included
  Z = 1, #in this design, all cases receive intervention 
  W = rbinom(N, 1, .8), # score on speech production test; pass/fail - potential covariate for later on. Ignore here.
  tau = round(runif(N, min = mineffect, max = maxeffect),0), # treatment effect - here specified to vary at random for each person from min to max. These values selected to give data that resemble Calder et al (though does depend on proportion of responders as well as max and min)
  #tau = 0,  #set tau to zero to see what results look like if treatment is ineffective for everyone.
  responder = rbinom(N, 1, p), #specifies which cases show treatment effect; this will be a proportion of people based on p.  NB this is a latent variable that we cannot observe, but we try to infer it from the data.
  U = 2*rnorm(N), #random error (U for unknown) #multiplied by 2 just to give realistic amount of variation across people. 
  r = .7, #estimated correlation between successive measures without any intervention
 
  Y1_1 = rbinom(N,10,startprob), #baseline % correct as binomial where startprob is prob of item correct
  Y2_1 = round(r*Y1_1+sqrt(1-r^2)*U,0), #Y1_1 is baseline measure, Y2_1 is 2nd baseline, correlates with Y1_1
  Y1_2 = round(r*Y2_1+sqrt(1-r^2)*U,0), #Y1_2 is first post-intervention measure, correlates with Y2_1
  Y2_2 = round(r*Y1_2+sqrt(1-r^2)*U,0), #Y2_2 is 2nd post-intervention measure, correlates with Y1_2
  #Now do potential outcomes for post-intervention measures - everyone treated, so Z is 1. 
  Y1_2_Z = Y1_2+Z*tau*responder, #treatment effect added to Y1_2
  Y2_2_Z = Y2_2+Z*tau*responder, #treatment effect added to Y2_2
  
  # ensure all values bounded between 0 and 10.
  Y2_1 = if_else(Y2_1 > 10, 10, Y2_1),
  Y1_2_Z = if_else(Y1_2_Z>10, 10, Y1_2_Z),
  Y2_2_Z = if_else(Y2_2_Z>10, 10, Y2_2_Z),
  Y2_1 = if_else(Y2_1 < 0, 0, Y2_1),
  Y1_2_Z = if_else(Y1_2_Z<0, 0, Y1_2_Z),
  Y2_2_Z = if_else(Y2_2_Z<0, 0, Y2_2_Z),
  
  no_overlap = pmax(Y1_1, Y2_1) < pmin( Y1_2_Z, Y2_2_Z), #1 if values from pretest don't overlap with posttest.
  correctcat = ifelse(as.numeric(no_overlap)==responder,1,0), #1 if no_overlap measure correctly categorises as responder or nonresponder. The proportion of correctcat cases indicates how accurate the no_overlap criterion is at categorising people
  
  #We now compare with Busk and Serlin's measure, d2
   busk_d2 = ((Y1_2_Z+Y2_2_Z)-(Y1_1+Y2_1))/2/sqrt((var(Y1_1, Y2_1)+var(Y1_2_Z, Y2_2_Z))/2),
   buskcat = ifelse(busk_d2>1.65,1,0),  #value greater than 1.65 indicates sig increase in score
   correctbusk = ifelse(buskcat==responder,1,0) #1 if busk category and latent responder measure agree 
)
  
#If I want to see what is generated by M, I just type:
M()
#Or can allocate m <- M() to get dataframe m.
```

### Inquiry  
The second theoretical component of the design is the Inquiry, i.e. the question that we ask of the Model. In an RCT, it is usually, what is the average effect of intervention. In a SCED, however, we ask which individuals benefited from the intervention.  We have to decide on a criterion to use to judge who improved. This has been a topic of considerable debate in the literature on SCEDs. If we have only two baseline measures and two post-intervention measures, as in this example, we could use a very simple index, i.e. whether there is any overlap between the two sets of scores. An alternative option would be to take a numerical cutoff, e.g. whether the mean post-intervention cutoff is at least 25% higher than the pre-intervention cutoff.  We show here the Inquiry for the first of these options.

```{r inquiryA, echo=T}

#Our interest is in what proportion of cases are corrrectly categorised by our categorisation rule.
#Catgorisation rule in this case is whether there is any overlap between the two pre-intervention scores and the two postintervention scores.
#We used this to infer whether the person is a responder or non-responder : the responder variable is a latent variable that determines outcomes but is not directly observable. 
#Only a proportion of people are responders. 
I <- declare_inquiry(
  correctID1 = mean(correctcat),
   correctID2 = mean(correctbusk)
  )


M() %>% I() #this displays estimand from M() - will differ on each run

#Or can set i <- I(M()) and then look at i
```

Both of these criteria for improvement might seem like rather inadequate and unsophisticated methods. The beauty of DeclareDesign is that it allows us to test whether such misgivings are justified, because we can see exactly how our Inquiry performs under various scenarios specified in the model. In particular, we can check:  
- How often does the method lead us to conclude that a person has improved with intervention when the null hypothesis is true, i.e. there is no benefit from intervention. This is the False Positive rate.  
- How often does the method correctly indicate which persons have improved with intervention when tau has been set to a nonzero value. This is analogous to the power of the approach.  

We can also check how far the performance of the method improves if we modify the Model to include more datapoints, e.g. if there are 3 or 4 measures at baseline and/or post-intervention.  

### Data Strategy  

```{r dataA,echo=T}
# Sample from the population defined in M
Sall <- 23

sampling_handler = function(data)
  data %>%
  dplyr::filter(Y1_1 < 10) %>%
  dplyr::filter(complete_rs(n(), Sall)==1)
  

D <- declare_sampling(handler = sampling_handler)

#to see a sample of data type
# D(m)
```

### Answer strategy  
The answer strategy is applied to the empirical data to answer the inquiry. 
There are numerous ways in which an answer can be determined, most of which involve applying a standard statistical test. In our simple example, however, we use a very crude index and so do not need to do more than count.  

```{r answerA,echo=T}
A <- declare_estimator(
  correctcat ~ 1, model = lm_robust, inquiry = "correctID1") #This is responder/nonresponder categorisation correct
A2 <-  declare_estimator(
  correctbusk ~ 1, model = lm_robust, inquiry = "correctID2") #This is categorisation by busk


#to see results of this step:
A(D(m))
A2(D(m))


```


```{r designA,echo=T}
designA2 <-
  M +
  I +
  D +
  A +
  A2

mydraw <- draw_data(designA2) #single simulated dataset

#Plot results of single draw to check whether they look like real data
yvals <- c(mean(mydraw$Y1_1),mean(mydraw$Y2_1),mean(mydraw$Y1_2_Z),mean(mydraw$Y2_2_Z))
plot(1:4,yvals,type='b',xaxt = "n",ylab="Mean score",xlab="",ylim=c(0,8),main='Sample plot of results')
axis(1, at = 1:4)
text(1.5,7,'Pre-intervention',cex=.7)
text(3.5,7,'Post-intervention',cex=.7)


## How are overlap and responder status related
## Of course overall depends on both treatment effects and responder status

#This table indicates how many improved, as well as how good correspondence is between the true (latent) status of individuals as responders, and their identification as an improved case (with no overlap). 
myt <- table(mydraw$responder, mydraw$no_overlap)
row.names(myt)<-c('Non-responder','Responder')
colnames(myt)<-c('Overlap','No overlap')
myt

myt <- table(mydraw$responder, mydraw$buskcat)
row.names(myt)<-c('Non-responder','Responder')
colnames(myt)<-c('Busk0','Busk1')
myt

draw_estimates(designA2) %>% kable()



## Below is the code that makes the default diagnosands.
# You can use these as a model when writing your own diagnosands.

# default_diagnosands <- declare_diagnosands(
# bias = mean(estimate - estimand),
# rmse = sqrt(mean((estimate - estimand) ^ 2)),
# power = mean(p.value < alpha),
# coverage = mean(estimand <= conf.high & estimand >= conf.low),
# mean_estimate = mean(estimate),
# sd_estimate = sd(estimate),
# mean_se = mean(std.error),
# type_s_rate = mean((sign(estimate) != sign(estimand))[p.value < alpha]),
# mean_estimand = mean(estimand)
# )

declare_diagnosands(
bias = mean(estimate - estimand),
rmse = sqrt(mean((estimate - estimand) ^ 2)),
power = mean(p.value < alpha),
mean_estimate = mean(estimate),
sd_estimate = sd(estimate),
mean_estimand = mean(estimand))


diagnose_design(designA2) %>% reshape_diagnosis() %>% kable()


#To see repeated estimates from design:
simulations <- simulate_design(designA, sims = 100)

```

Now we can tweak aspects of model to see the effect

```{r tweakmodel}
designs <- redesign(designA,
                    p=seq(.3,.9,.2),
                    startprob=seq(.1,.5,.1))
diagnosis <- diagnose_designs(designs)

```


